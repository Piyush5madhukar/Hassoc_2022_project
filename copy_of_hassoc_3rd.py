# -*- coding: utf-8 -*-
"""Copy of HASSOC_3rd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yiqD9_JXUB9Bz0T8F2rXevZdIsykgu3U
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from glob import glob
import re
import json
import sys 

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
# import stemmer as hindi_stemmer

!pip install stanza

import stanza

stanza.download('ta')

nlp = stanza.Pipeline(lang='mr', processors='tokenize,mwt,pos,lemma')

df = pd.read_csv('/content/drive/MyDrive/hssoc/MOLD_train/MOLD_train.tsv',sep = '\t')

df_test = pd.read_csv('/content/drive/MyDrive/hasoc_test/marathi_test_without_labels.csv')

df_test

df

df.subtask_a.value_counts()

df.subtask_b.value_counts()

df.subtask_c.value_counts()

df= df[df['tweet'].isnull()==False]

df= df[df['subtask_b'] == 'TIN']

df= df[df['subtask_b'].isnull()==False]

d_b = {'TIN' : 1 , 'UNT' : 2, 0 : 0}

d_c = {0 : 0 , 'IND' : 1 , 'GRP' : 2, 'OTH' : 3}

d_a = {'NOT' : 0 , 'OFF' : 1}

df['subtask_b'] = df['subtask_b'].fillna(0)

df['subtask_c'] = df['subtask_c'].fillna(0)

df.subtask_c = df.subtask_c.map(d_c)

df.subtask_a = df.subtask_a.map(d_a)

df.subtask_b = df.subtask_b.map(d_b)

regex_for_english_hindi_emojis="[^a-zA-Z#\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF\u0900-\u097F]"
def clean_tweet(tweet):
    tweet = re.sub(r"@[A-Za-z0-9]+",' ', tweet)
    tweet = re.sub(r"https?://[A-Za-z0-9./]+",' ', tweet)
    tweet = re.sub(regex_for_english_hindi_emojis,' ', tweet)
    tweet = re.sub("RT ", " ", tweet)
    tweet = re.sub("\n", " ", tweet)
    tweet = re.sub(r" +", " ", tweet)

    return tweet

tweets = list(df.tweet)

tweets

cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]

tweets[0]

labels_a = df.subtask_a

labels_b = df.subtask_b

labels_c = df.subtask_c

labels_a

labels_b

vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(cleaned_tweets)
# X = X.todense()

"""# ***task a***"""

X_train, X_val, y_train, y_val = train_test_split(X, labels_a, test_size=0.2, random_state=42)

classifier = KNeighborsClassifier(4)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_val)
print("With K-nearest Neighbour:")
print(classification_report(y_val, y_pred))

y_pred = classifier.predict(X_val)
print("With K-nearest Neighbour:")
print(classification_report(y_val, y_pred))

"""LogisticRegression """

reg = LogisticRegression(C = 25,penalty ='l2'  , solver = 'newton-cg' , random_state = 42)

reg.fit(X, labels_a)

y_pred = reg.predict(X_val)
print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

# f = open("result/lg_agn.txt",'w')
# f.write(f"LogisticRegression(C = {0},tol = {0},penalty ={0})")
# f.close()
from sklearn.metrics import f1_score
C = [1,5,25,50,100,200]
penalty = ["l2" , 'l1' ,'elasticnet']
solver = ['newton-cg', 'lbfgs', 'sag','liblinear','saga']
# l1_ratio = [0.0,0.3,0.5,0.8,1.0]

score = 0
for i in C:
  for k in penalty:
    for l in solver :
      print(i,k,l)
      try:
        model = LogisticRegression(C = i,penalty =k  , solver = l , random_state = 42)
        model.fit(X_train,y_train)
      except :
        continue
      pred_m = model.predict(X_val)
      f = f1_score(y_val,pred_m,average = 'macro')
      if(score<f):
          score = f
          # f = open("result/lg_agn.txt",'w')
          # f.write(f"LogisticRegression(C = {i},penalty = {k},solver={l})")
          # f.close()
          # joblib.dump(model,"result/agn_lg.joblib")
          print(i,k,l,score)
          # print(score)

"""SVM"""

from sklearn.svm import SVC

model3 = SVC(C = 10 ,kernel= 'rbf')
model3.fit(X,labels_a)

import joblib

joblib.dump(model3,'/content/drive/MyDrive/hasoc_test/svm_mold_subtask_a.joblib')

y_pred_a = model3.predict(X)

y_pred

df_test_final = pd.DataFrame(list(zip(df_test.id,tweets_test, y_pred)),columns =['id', 'tweet','subtask_a'])

d_a = { 0 : 'NOT' , 1 : 'OFF'}

df_test_final.subtask_a = df_test_final.subtask_a.map(d_a)

df_test_final

df_test.to_csv('/content/drive/MyDrive/hasoc_test/marathi_test_without_labels.csv')

y_pred = model3.predict(X)
print("With K-Nearest Neighbour:")
print(classification_report(labels_a, y_pred))

mx = 0
for kernel in ['linear', 'poly', 'rbf']:
  for C in [1,5,10,25,50,100,200]:
    # if(kernel=='poly'):
      # if(C != 200):
      for degree in [1,2,3,4]:
          model = SVC(C = C,kernel=kernel,degree=degree)
          model.fit(X_train,y_train)
          pred = model.predict(X_val)
          f = f1_score(y_val,pred,average='macro')
          if(mx<f):
              mx = max(mx,f)
              # s = open('result/gen_svm.txt' , 'w')
              # s.write(f"SVC(C= {C}, degree = {degree} , kernel = {kernel}) score={mx}")
              # s.close()
              # joblib.dump(model,'result/gen_svm.joblib')
              print(kernel,C,degree,mx)
          print(f"C = {C} , kernel = {kernel} , degree = {degree}")
    # else:
          model = SVC(C = C,kernel=kernel)
          model.fit(X_train,y_train)
          pred = model.predict(X_val)
          f = f1_score(y_val,pred,average='macro')
          if(mx<f):
              mx = max(mx,f)
              # s = open('result/gen_svm.txt' , 'w')
              # s.write(f"SVC(C= {C}, degree = {1} , kernal = {kernel}) score={mx}")
              # s.close()
              # joblib.dump(model,'result/gen_svm.joblib')
              print(kernel,C,degree,mx)
          print(f"C = {C} , kernel = {kernel}")

"""MNB"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB(alpha=0.06891)
model.fit(X_train,y_train)

y_pred = model.predict(X_val)
# print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score
score =0
for i in np.arange(1e-05,1e-01,1e-04):
  print(i)
  model = MultinomialNB(alpha=i)
  model.fit(X_train,y_train)
  pred = model.predict(X_val)
  f = f1_score(y_val,pred,average='macro')
  if(score<f):
    score = f
    # f = open("result/gen_mnb.txt",'w')
    # f.write(f"MultinomialNB(alpha = {i})")
    # f.close()
    # joblib.dump(model,'result/gen_mnb.joblib')
    print(i,score)

"""randomforest"""

from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import f1_score
max_features = ['auto', 'sqrt', 'log2']
max_depth = [4,5,6,7,8]
criterion =['gini', 'entropy']

score=0
for i in max_features:
  for j in max_depth:
    for k in criterion:
      print(i,j,k)
      model=RandomForestClassifier(max_features=i,max_depth=j,criterion=k)
      model.fit(X_train,y_train)
      pred=model.predict(X_val)
      f=f1_score(y_val,pred,average='macro')
      if(score<f):
        score=f
        # f=open("result/gen_rf.txt",'w')
        # f.write(f"RandomForestClassifier(C={i},tol={j},penalty={k}) score={score}")
        # f.close()
        # joblib.dump(model,'result/gen_rf.joblib')
        print(i,j,k,score)

model=RandomForestClassifier(class_weight= classweight)
model.fit(X_train,y_train)

y_pred = model.predict(X_val)
# print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

"""# **task b**"""

#applying class weight
from sklearn.utils import compute_class_weight
classweight = dict(zip([0,1,2],compute_class_weight(class_weight = 'balanced',y=df['subtask_b'].values,classes=[0,1,2])))
classweight

X_train, X_val, y_train, y_val = train_test_split(X, labels_b, test_size=0.2, random_state=42)

classifier = KNeighborsClassifier(4)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_val)
print("With K-nearest Neighbour:")
print(classification_report(y_val, y_pred))

"""LogisticRegression """



reg = LogisticRegression(C = 50,penalty ='l1'  , solver = 'saga' , random_state = 42)

reg.fit(X_train,y_train)

y_pred = reg.predict(X_val)
print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

y_pred_b = reg.predict(X_test)

y_pred_b_final = y_pred_b

range(len(labels_a_test)-1)

for i in range(len(labels_a_test)):
  if(labels_a_test[i] == 0):
    y_pred_b_final[i] = 0
  # print(i)

len(y_pred_b_final)

len(y_pred_b)



# f = open("result/lg_agn.txt",'w')
# f.write(f"LogisticRegression(C = {0},tol = {0},penalty ={0})")
# f.close()
from sklearn.metrics import f1_score
C = [1,5,25,50,100,200]
penalty = ["l2" , 'l1' ,'elasticnet']
solver = ['newton-cg', 'lbfgs', 'sag','liblinear','saga']
# l1_ratio = [0.0,0.3,0.5,0.8,1.0]

score = 0
for i in C:
  for k in penalty:
    for l in solver :
      print(i,k,l)
      try:
        model = LogisticRegression(C = i,penalty =k  , solver = l , random_state = 42, class_weight = classweight)
        model.fit(X_train,y_train)
      except :
        continue
      pred_m = model.predict(X_val)
      f = f1_score(y_val,pred_m,average = 'macro')
      if(score<f):
          score = f
          # f = open("result/lg_agn.txt",'w')
          # f.write(f"LogisticRegression(C = {i},penalty = {k},solver={l})")
          # f.close()
          # joblib.dump(model,"result/agn_lg.joblib")
          print(i,k,l,score)
          # print(score)

"""svm"""

from sklearn.svm import SVC

model3 = SVC(C = 100 ,kernel= 'linear')
model3.fit(X_train,y_train)

y_pred = model3.predict(X_val)
print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

mx = 0
for kernel in ['linear', 'poly', 'rbf']:
  for C in [1,5,10,25,50,100,200]:
    # if(kernel=='poly'):
      # if(C != 200):
      for degree in [1,2,3,4]:
          model = SVC(C = C,kernel=kernel,degree=degree,class_weight = classweight)
          model.fit(X_train,y_train)
          pred = model.predict(X_val)
          f = f1_score(y_val,pred,average='macro')
          if(mx<f):
              mx = max(mx,f)
              # s = open('result/gen_svm.txt' , 'w')
              # s.write(f"SVC(C= {C}, degree = {degree} , kernel = {kernel}) score={mx}")
              # s.close()
              # joblib.dump(model,'result/gen_svm.joblib')
              print(kernel,C,degree,mx)
          print(f"C = {C} , kernel = {kernel} , degree = {degree}")
    # else:
          model = SVC(C = C,kernel=kernel,degree = degree, class_weight = classweight)
          model.fit(X_train,y_train)
          pred = model.predict(X_val)
          f = f1_score(y_val,pred,average='macro')
          if(mx<f):
              mx = max(mx,f)
              # s = open('result/gen_svm.txt' , 'w')
              # s.write(f"SVC(C= {C}, degree = {1} , kernal = {kernel}) score={mx}")
              # s.close()
              # joblib.dump(model,'result/gen_svm.joblib')
              print(kernel,C,degree,mx)
          print(f"C = {C} , kernel = {kernel}")

score

"""mnb"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB(alpha=1e-05)
model.fit(X_train,y_train)

y_pred = model.predict(X_val)
# print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score
score =0
for i in np.arange(1e-05,1e-01,1e-04):
  print(i)
  model = MultinomialNB(alpha=i)
  model.fit(X_train,y_train)
  pred = model.predict(X_val)
  f = f1_score(y_val,pred,average='macro')
  if(score<f):
    score = f
    # f = open("result/gen_mnb.txt",'w')
    # f.write(f"MultinomialNB(alpha = {i})")
    # f.close()
    # joblib.dump(model,'result/gen_mnb.joblib')
    print(i,score)

"""randomforest"""

from sklearn.ensemble import RandomForestClassifier

model=RandomForestClassifier()
model.fit(X_train,y_train)

y_pred = model.predict(X_val)
# print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

from sklearn.metrics import f1_score
max_features = ['auto', 'sqrt', 'log2']
max_depth = [4,5,6,7,8]
criterion =['gini', 'entropy']

score=0
for i in max_features:
  for j in max_depth:
    for k in criterion:
      print(i,j,k)
      model=RandomForestClassifier(max_features=i,max_depth=j,criterion=k)
      model.fit(X_train,y_train)
      pred=model.predict(X_val)
      f=f1_score(y_val,pred,average='macro')
      if(score<f):
        score=f
        # f=open("result/gen_rf.txt",'w')
        # f.write(f"RandomForestClassifier(C={i},tol={j},penalty={k}) score={score}")
        # f.close()
        # joblib.dump(model,'result/gen_rf.joblib')
        print(i,j,k,score)

"""# **task_c**"""

#applying class weight
from sklearn.utils import compute_class_weight
classweight = dict(zip([1,2,3],compute_class_weight(class_weight = 'balanced',y=df['subtask_c'].values,classes=[1,2,3])))
classweight

X_train, X_val, y_train, y_val = train_test_split(X, labels_c, test_size=0.2, random_state=42)

"""LogisticRegression """

reg = LogisticRegression(C = 1,penalty ='l2'  , solver = 'liblinear' , random_state = 42, class_weight= classweight)

reg.fit(X,labels_c)

y_pred_c = reg.predict(X_test)

y_pred_c

y_pred_c_final = y_pred_c

y_pred_c_final

for i in range(len(label_b_test)) :
  if(label_b_test[i] != 1) :
    y_pred_c_final[i] = 0

y_pred = reg.predict(X_val)
print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

# f = open("result/lg_agn.txt",'w')
# f.write(f"LogisticRegression(C = {0},tol = {0},penalty ={0})")
# f.close()
from sklearn.metrics import f1_score
C = [1,5,25,50,100,200]
penalty = ["l2" , 'l1' ,'elasticnet']
solver = ['newton-cg', 'lbfgs', 'sag','liblinear','saga']
# l1_ratio = [0.0,0.3,0.5,0.8,1.0]

score = 0
for i in C:
  for k in penalty:
    for l in solver :
      print(i,k,l)
      try:
        model = LogisticRegression(C = i,penalty =k  , solver = l , random_state = 42, class_weight = classweight)
        model.fit(X_train,y_train)
      except :
        continue
      pred_m = model.predict(X_val)
      f = f1_score(y_val,pred_m,average = 'macro')
      if(score<f):
          score = f
          # f = open("result/lg_agn.txt",'w')
          # f.write(f"LogisticRegression(C = {i},penalty = {k},solver={l})")
          # f.close()
          # joblib.dump(model,"result/agn_lg.joblib")
          print(i,k,l,score)
          # print(score)



"""SVM"""

from sklearn.svm import SVC

model3 = SVC(C = 5 ,kernel= 'rbf')
model3.fit(X_train,y_train)

y_pred = model3.predict(X_val)
print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

mx = 0
for kernel in ['linear', 'poly', 'rbf']:
  for C in [1,5,10,25,50,100,200]:
    # if(kernel=='poly'):
      # if(C != 200):
      for degree in [1,2,3,4]:
          model = SVC(C = C,kernel=kernel,degree=degree)
          model.fit(X_train,y_train)
          pred = model.predict(X_val)
          f = f1_score(y_val,pred,average='macro')
          if(mx<f):
              mx = max(mx,f)
              # s = open('result/gen_svm.txt' , 'w')
              # s.write(f"SVC(C= {C}, degree = {degree} , kernel = {kernel}) score={mx}")
              # s.close()
              # joblib.dump(model,'result/gen_svm.joblib')
              print(kernel,C,degree,mx)
          print(f"C = {C} , kernel = {kernel} , degree = {degree}")
    # else:
          model = SVC(C = C,kernel=kernel,degree= degree)
          model.fit(X_train,y_train)
          pred = model.predict(X_val)
          f = f1_score(y_val,pred,average='macro')
          if(mx<f):
              mx = max(mx,f)
              # s = open('result/gen_svm.txt' , 'w')
              # s.write(f"SVC(C= {C}, degree = {1} , kernal = {kernel}) score={mx}")
              # s.close()
              # joblib.dump(model,'result/gen_svm.joblib')
              print(kernel,C,degree,mx)
          print(f"C = {C} , kernel = {kernel}")

"""MNB"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB(alpha=0.03681)
model.fit(X_train,y_train)

y_pred = model.predict(X_val)
# print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score
score =0
for i in np.arange(1e-05,1e-01,1e-04):
  print(i)
  model = MultinomialNB(alpha=i)
  model.fit(X_train,y_train)
  pred = model.predict(X_val)
  f = f1_score(y_val,pred,average='macro')
  if(score<f):
    score = f
    # f = open("result/gen_mnb.txt",'w')
    # f.write(f"MultinomialNB(alpha = {i})")
    # f.close()
    # joblib.dump(model,'result/gen_mnb.joblib')
    print(i,score)

score

"""randomforest"""

from sklearn.ensemble import RandomForestClassifier

model=RandomForestClassifier(class_weight= classweight)
model.fit(X_train,y_train)

y_pred = model.predict(X_val)
# print("With K-Nearest Neighbour:")
print(classification_report(y_val, y_pred))

from sklearn.metrics import f1_score
max_features = ['auto', 'sqrt', 'log2']
max_depth = [4,5,6,7,8]
criterion =['gini', 'entropy']

score=0
for i in max_features:
  for j in max_depth:
    for k in criterion:
      print(i,j,k)
      model=RandomForestClassifier(max_features=i,max_depth=j,criterion=k)
      model.fit(X_train,y_train)
      pred=model.predict(X_val)
      f=f1_score(y_val,pred,average='macro')
      if(score<f):
        score=f
        # f=open("result/gen_rf.txt",'w')
        # f.write(f"RandomForestClassifier(C={i},tol={j},penalty={k}) score={score}")
        # f.close()
        # joblib.dump(model,'result/gen_rf.joblib')
        print(i,j,k,score)